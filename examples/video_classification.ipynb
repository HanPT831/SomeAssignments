{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-woWIqv0k-6c"
      },
      "source": [
        "# Fine-tuning for Video Classification with ü§ó Transformers\n",
        "\n",
        "This notebook shows how to fine-tune a pre-trained Vision model for Video Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder and fine-tune the model altogether on a labeled dataset.\n",
        "\n",
        "\n",
        "## Dataset\n",
        "\n",
        "This notebook uses a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). We'll be using a subset of the dataset to keep the runtime of the tutorial short. The subset was prepared using [this notebook](https://drive.google.com/file/d/1tTScjnyiKrBz84jKe1H_hPGGXffAZuxX/view?usp=sharing) following [this guide](https://www.tensorflow.org/tutorials/load_data/video).\n",
        "\n",
        "## Model\n",
        "\n",
        "We'll fine-tune the [VideoMAE model](https://huggingface.co/docs/transformers/model_doc/videomae), which was pre-trained on the [Kinetics 400 dataset](https://www.deepmind.com/open-source/kinetics). You can find the other variants of VideoMAE available on ü§ó Hub [here](https://huggingface.co/models?search=videomae). You can also extend this notebook to use other video models such as [X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip#transformers.XCLIPVisionModel).\n",
        "\n",
        "**Note** that for models where there's no classification head already available you'll have to manually attach it (randomly initialized). But this is not the case for VideoMAE since we already have a [`VideoMAEForVideoClassification`](https://huggingface.co/docs/transformers/model_doc/xclip#transformers.XCLIPVisionModel) class.\n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "This notebook leverages [TorchVision's](https://pytorch.org/vision/stable/transforms.html) and [PyTorchVideo's](https://pytorchvideo.org/) transforms for applying data preprocessing transformations including data augmentation.\n",
        "\n",
        "---\n",
        "\n",
        "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SHEFxC8AoBHx"
      },
      "outputs": [],
      "source": [
        "model_ckpt = \"MCG-NJU/videomae-base\" # pre-trained model from which to fine-tune\n",
        "batch_size = 8 # batch size for training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGpPMMPTn4jv"
      },
      "source": [
        "Before we start, let's install the `pytorchvideo`, `transformers`, and `evaluate` libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acKsyA8D1tg",
        "outputId": "c7a72e0a-27ce-4964-a250-6c8ff42f3d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/132.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorchvideo transformers evaluate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UajzcKwMoOsE"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your token:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlRg6wgKoRfe"
      },
      "source": [
        "Then you need to install Git-LFS to upload your model checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aTqcdBnoSEZM"
      },
      "outputs": [],
      "source": [
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmp8xcoQr_0x"
      },
      "source": [
        "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pqlFh2Acr_0x"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "\n",
        "send_example_telemetry(\"video_classification_notebook\", framework=\"pytorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I1tApJsoX_K"
      },
      "source": [
        "## Fine-tuning a model on a video classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02SWnr4Poauk"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) vision models on a Video Classification dataset.\n",
        "\n",
        "Given a video, the goal is to predict an appropriate class for it, like \"archery\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgYB_Jd2oo_X"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKB9fV5oqli"
      },
      "source": [
        "Here we first download the subset archive and un-archive it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "1363b1b39aa644879c304cce686392e9",
            "2eb2f8f98e8e408eb2d44b6174dc61c9",
            "14e654de976c4e59ac3805f4aef80605",
            "68fb73f8fa9e4c5587db390768c06902",
            "c68c0a91c97641859c381f0e0ce4ee87",
            "ec7b5eadb3ab40ad9f2bf1bfcf7422af",
            "02da0562b2d64314aeb8a368cf224a08",
            "f53deb79be5c40b6a00948969b4f1a1e",
            "8dc62a08141b479bb2aef681af472cde",
            "812352feeeec47829391aacbd2bccb3c",
            "2757c1881dff427faf7a7b697b9c1066"
          ]
        },
        "id": "GlrpwCCWEzE2",
        "outputId": "8dbb7bbc-3815-4bd1-ed8b-2d4098e86298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "UCF101_subset.tar.gz:   0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1363b1b39aa644879c304cce686392e9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
        "filename = \"UCF101_subset.tar.gz\"\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oUEtAxdrGYoF"
      },
      "outputs": [],
      "source": [
        "!tar xf {file_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zcaozo0oywM"
      },
      "source": [
        "Now, let's investigate what is inside the archive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26zbQGoTHPBY",
        "outputId": "42782e20-4254-4410-c958-76afc2bf6fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UCF101_subset\n",
            "UCF101_subset/val\n",
            "UCF101_subset/val/Basketball\n",
            "UCF101_subset/val/Basketball/v_Basketball_g20_c02.avi\n",
            "UCF101_subset/val/Basketball/v_Basketball_g10_c03.avi\n"
          ]
        }
      ],
      "source": [
        "dataset_root_path = \"UCF101_subset\"\n",
        "\n",
        "!find {dataset_root_path} | head -5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXvxyNupqdJD"
      },
      "source": [
        "Broadly, `dataset_root_path` is organized like so:\n",
        "\n",
        "```bash\n",
        "UCF101_subset/\n",
        "    train/\n",
        "        BandMarching/\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        Archery\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        ...\n",
        "    val/\n",
        "        BandMarching/\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        Archery\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        ...\n",
        "    test/\n",
        "        BandMarching/\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        Archery\n",
        "            video_1.mp4\n",
        "            video_2.mp4\n",
        "            ...\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_AF92NTo5aw"
      },
      "source": [
        "Let's now count the number of total videos we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oJMAZqE-HavC"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_root_path = pathlib.Path(dataset_root_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZRB1yJQHm5W",
        "outputId": "79db113f-4e52-49b2-d68b-2b52e567cf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos: 405\n"
          ]
        }
      ],
      "source": [
        "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.avi\")))\n",
        "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.avi\")))\n",
        "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.avi\")))\n",
        "video_total = video_count_train + video_count_val + video_count_test\n",
        "print(f\"Total videos: {video_total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNUCBLe6HoLY",
        "outputId": "a465e147-9c8b-4d17-add5-febce574b5bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('UCF101_subset/train/Basketball/v_Basketball_g01_c05.avi'),\n",
              " PosixPath('UCF101_subset/train/Basketball/v_Basketball_g09_c02.avi'),\n",
              " PosixPath('UCF101_subset/train/Basketball/v_Basketball_g03_c06.avi'),\n",
              " PosixPath('UCF101_subset/train/Basketball/v_Basketball_g15_c07.avi'),\n",
              " PosixPath('UCF101_subset/train/Basketball/v_Basketball_g17_c04.avi')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "all_video_file_paths = (\n",
        "    list(dataset_root_path.glob(\"train/*/*.avi\"))\n",
        "    + list(dataset_root_path.glob(\"val/*/*.avi\"))\n",
        "    + list(dataset_root_path.glob(\"test/*/*.avi\"))\n",
        ")\n",
        "all_video_file_paths[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjyXBiO9dQTu"
      },
      "source": [
        "The video paths, when `sorted`, appear like so:\n",
        "\n",
        "```py\n",
        "...\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n",
        "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n",
        "...\n",
        " ```\n",
        "\n",
        "We notice that there are video clips belonging to the same group / scene where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi` and `v_ApplyEyeMakeup_g07_c06.avi`, for example.\n",
        "\n",
        "\n",
        " For the validation and evaluation splits, we wouldn't want to have video clips from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage). The subset that we're using in this tutorial takes this information into account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NFY4jkcpA9N"
      },
      "source": [
        "Next up, we derive the set of labels we have in the dataset. Let's also create two dictionaries that'll be helpful when initializing the model:\n",
        "\n",
        "* `label2id`: maps the class names to integers.\n",
        "* `id2label`: maps the integers to class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeWXvJSDHpbw",
        "outputId": "cb9af437-ea59-4ddf-a469-e5abcbfc78e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
          ]
        }
      ],
      "source": [
        "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
        "label2id = {label: i for i, label in enumerate(class_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(f\"Unique classes: {list(label2id.keys())}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ_Q_ysnq_q3"
      },
      "source": [
        "We've got 10 unique classes. For each class we have 30 videos in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6qmNcvQpXwW"
      },
      "source": [
        "### Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l8MehCcpe70"
      },
      "source": [
        "In the next cell, we initialize a video classification model where the encoder is initialized with the pre-trained parameters and the classification head is randomly initialized. We also initialize the feature extractor associated to the model. This will come in handy during writing the preprocessing pipeline for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "aac3d030d04f46dab650c46be73742e9",
            "e8e95293d2a64794990d9bf7baed574a",
            "c92d7b5f70be49dc9517320e3b68ccd8",
            "fc0dea2d2fe543c99e91213f06bff086",
            "74be2b5c59354d4183f6eb79e031a29a",
            "b092fd4502dc48d4bfc495acc8ed9970",
            "400002c938c046c98940a7cc353f75d9",
            "f82cc11214784bd192f52a400042f6f9",
            "6ae9ee7cd3854ce4ab827f09786b9833",
            "9385c2381ce647879105a0517b45304a",
            "fb93fef6438c45228caeac789837b49d",
            "b9c93c7baea74278b1383648615f663d",
            "ed535130915946e69820e55ff60a6c31",
            "fedddfab7d8241c2bd123b58470d472f",
            "9cc9d3e48b4e4270a6ba7df0fbafc9c5",
            "ede2fde912ec4da78edfd1da9c683bd8",
            "5e27e17290d3459a92a101e8a97f860f",
            "99a527c3700c4eb3929b4f36968b7c24",
            "4959cacabdec4d16a35d7bc754146531",
            "6e8881830aac4be3908b5b206fd78691",
            "514f929928d0490c83ffe4b39b48cfd3",
            "659ac7ede6ba4197b953754c97537882",
            "265db1817c304fe5b7d77808a53dc10f",
            "c8acb16968b64b71a480503b2bc3b527",
            "b4012293c0c649c4893b9eb711dca0d4",
            "96e7b2da779f4ebb82ab47b1ee1658a4",
            "a765714fd9c249ff95eace72695db977",
            "6aaa7828d31946678818cdd0761d349d",
            "d356a1f0cb70435e8ddff2558accdcc8",
            "c86e8f2144cc4ec1bb549419dfc5ed9c",
            "c5c6335bebfd4571814468ba5bde0469",
            "fb56d881585e42a0a447331ba58c1787",
            "2ac89b7ca8384145b9e405626992f1df"
          ]
        },
        "id": "vv2Sjp94MW2f",
        "outputId": "c21d6ac4-1a82-4a46-9e4b-700b81d6db8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aac3d030d04f46dab650c46be73742e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9c93c7baea74278b1383648615f663d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "265db1817c304fe5b7d77808a53dc10f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, VideoMAEModel\n",
        "\n",
        "\n",
        "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
        "model = VideoMAEModel.from_pretrained(\n",
        "    model_ckpt,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5AOoVryp2Qj"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some other (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP9lWPmaawJT"
      },
      "source": [
        "**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics` and it obtains much better performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p27JMEEHp5DZ"
      },
      "source": [
        "### Constructing the datasets for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnNa_UIDp-hr"
      },
      "source": [
        "For preprocessing the videos, we'll leverage the [PyTorch Video library](https://pytorchvideo.org/). We start by importing the dependencies we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7Mkg_DzsMsSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c89cea8e-2e7b-4ca8-cb0d-ee0d829a6103"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.transforms.functional_tensor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-3436037420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorchvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from pytorchvideo.transforms import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mApplyTransformToKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mNormalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAugMix\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCutMix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMixVideo\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrand_augment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pytorchvideo.transforms.augmentations import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_AUGMENTATION_MAX_LEVEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mAugmentTransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorchvideo/transforms/augmentations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pytorchvideo.data\n",
        "\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    Normalize,\n",
        "    RandomShortSideScale,\n",
        "    RemoveKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        ")\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Lambda,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT_RZTmFqX9I"
      },
      "source": [
        "For the training dataset transformations, we use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, we keep the transformation chain the same except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorch Video](https://pytorchvideo.org).  \n",
        "\n",
        "We'll use the `image_processor` associated with the pre-trained model to obtain the following information:\n",
        "\n",
        "* Image mean and standard deviation with which the video frame pixels will be normalized.\n",
        "* Spatial resolution to which the video frames will be resized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LwNh61AxMu6Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "mean = image_processor.image_mean\n",
        "std = image_processor.image_std\n",
        "if \"shortest_edge\" in image_processor.size:\n",
        "    height = width = image_processor.size[\"shortest_edge\"]\n",
        "else:\n",
        "    height = image_processor.size[\"height\"]\n",
        "    width = image_processor.size[\"width\"]\n",
        "resize_to = (height, width)\n",
        "\n",
        "num_frames_to_sample = model.config.num_frames\n",
        "sample_rate = 4\n",
        "fps = 30\n",
        "clip_duration = num_frames_to_sample * sample_rate / fps\n",
        "\n",
        "\n",
        "# Training dataset transformations.\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        ApplyTransformToKey(\n",
        "            key=\"video\",\n",
        "            transform=Compose(\n",
        "                [\n",
        "                    UniformTemporalSubsample(num_frames_to_sample),\n",
        "                    Lambda(lambda x: x / 255.0),\n",
        "                    Normalize(mean, std),\n",
        "                    RandomShortSideScale(min_size=256, max_size=320),\n",
        "                    RandomCrop(resize_to),\n",
        "                    RandomHorizontalFlip(p=0.5),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Training dataset.\n",
        "train_dataset = pytorchvideo.data.Ucf101(\n",
        "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
        "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
        "    decode_audio=False,\n",
        "    transform=train_transform,\n",
        ")\n",
        "\n",
        "# Validation and evaluation datasets' transformations.\n",
        "val_transform = Compose(\n",
        "    [\n",
        "        ApplyTransformToKey(\n",
        "            key=\"video\",\n",
        "            transform=Compose(\n",
        "                [\n",
        "                    UniformTemporalSubsample(num_frames_to_sample),\n",
        "                    Lambda(lambda x: x / 255.0),\n",
        "                    Normalize(mean, std),\n",
        "                    Resize(resize_to),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Validation and evaluation datasets.\n",
        "val_dataset = pytorchvideo.data.Ucf101(\n",
        "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
        "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
        "    decode_audio=False,\n",
        "    transform=val_transform,\n",
        ")\n",
        "\n",
        "test_dataset = pytorchvideo.data.Ucf101(\n",
        "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
        "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
        "    decode_audio=False,\n",
        "    transform=val_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlMg2MjF40xI"
      },
      "source": [
        "**Note**: The above dataset pipelines are taken from the [official PyTorch Video example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorch Video dataset. So, if you wanted to use a custom dataset not supported off-the-shelf by PyTorch Video, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-39840bVLtc",
        "outputId": "216cff81-f5c9-44e5-d164-7dc6b90af4ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 30, 75)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# We can access the `num_videos` argument to know the number of videos we have in the\n",
        "# dataset.\n",
        "train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8XnMMqsPKE"
      },
      "source": [
        "Let's now take a preprocessed video from the dataset and investigate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9VwjeQPM0nH",
        "outputId": "2b3a57c9-f3cb-4e2f-e8dd-c22380c27926"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "sample_video = next(iter(train_dataset))\n",
        "sample_video.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk-DcRJuM3lS",
        "outputId": "00322d69-1e64-44ff-b4f3-225c7cf2fe9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video torch.Size([3, 16, 224, 224])\n",
            "video_name v_Archery_g19_c03.avi\n",
            "video_index 82\n",
            "clip_index 0\n",
            "aug_index 0\n",
            "label 2\n",
            "Video label: Archery\n"
          ]
        }
      ],
      "source": [
        "def investigate_video(sample_video):\n",
        "    \"\"\"Utility to investigate the keys present in a single video sample.\"\"\"\n",
        "    for k in sample_video:\n",
        "        if k == \"video\":\n",
        "            print(k, sample_video[\"video\"].shape)\n",
        "        else:\n",
        "            print(k, sample_video[k])\n",
        "\n",
        "    print(f\"Video label: {id2label[sample_video[k]]}\")\n",
        "\n",
        "\n",
        "investigate_video(sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua8Sd_a1sZqS"
      },
      "source": [
        "We can also visualize the preprocessed videos for easier debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8lUv_VvPIRP"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "def unnormalize_img(img):\n",
        "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
        "    img = (img * std) + mean\n",
        "    img = (img * 255).astype(\"uint8\")\n",
        "    return img.clip(0, 255)\n",
        "\n",
        "\n",
        "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
        "    \"\"\"Prepares a GIF from a video tensor.\n",
        "\n",
        "    The video tensor is expected to have the following shape:\n",
        "    (num_frames, num_channels, height, width).\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    for video_frame in video_tensor:\n",
        "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
        "        frames.append(frame_unnormalized)\n",
        "    kargs = {\"duration\": 0.25}\n",
        "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
        "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
        "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
        "    gif_filename = create_gif(video_tensor, gif_name)\n",
        "    return Image(filename=gif_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJIPKvyqQBqL"
      },
      "outputs": [],
      "source": [
        "video_tensor = sample_video[\"video\"]\n",
        "display_gif(video_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lcHvCL1silT"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uc1ibDftk1z"
      },
      "source": [
        "We'll leverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  ü§ó Transformers for training the model. To instantiate a `Trainer`, we will need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on ü§ó Hub.\n",
        "\n",
        "Most of the training arguments are pretty self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('video' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4DumIl1Qpdb"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model_name = model_ckpt.split(\"/\")[-1]\n",
        "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
        "num_epochs = 4\n",
        "\n",
        "args = TrainingArguments(\n",
        "    new_model_name,\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Agf6aabAFDj"
      },
      "source": [
        "There's no need to define `max_steps` when instantiating `TrainingArguments`. Since the dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__()` method we had to specify `max_steps`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNA25v_P2pqf"
      },
      "source": [
        "Next, we need to define a function for how to compute the metrics from the predictions, which will just use the `metric` we'll load now. The only preprocessing we have to do is to take the argmax of our predicted logits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWZIHEDTSddw"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRzsIQKXR-HM"
      },
      "outputs": [],
      "source": [
        "# the compute_metrics function takes a Named Tuple as input:\n",
        "# predictions, which are the logits of the model as Numpy arrays,\n",
        "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjVzEJuEawJX"
      },
      "source": [
        "**A note on evaluation**:\n",
        "\n",
        "In the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs0EnSja2yFk"
      },
      "source": [
        "We also define a `collate_fn`, which will be used to batch examples together.\n",
        "Each batch consists of 2 keys, namely `pixel_values` and `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UVBOMWZGSowx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"The collation function to be used by `Trainer` to prepare data batches.\"\"\"\n",
        "    # permute to (num_frames, num_channels, height, width)\n",
        "    pixel_values = torch.stack(\n",
        "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
        "    )\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW--x7cM22HB"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED4D-fdDStrD"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltokP9mO9pjI"
      },
      "source": [
        "You might wonder why we pass along the `image_processor` as a tokenizer when we already preprocessed our data. This is only to make sure the feature extractor configuration file (stored as JSON) will also be uploaded to the repo on the hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j6VNsGP97LG"
      },
      "source": [
        "Now we can finetune our model by calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG7gSxW7Y3mH"
      },
      "outputs": [],
      "source": [
        "train_results = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPyg-jgd3YPN"
      },
      "source": [
        "We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9C-eDjZkoQC"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq9DR-qwkfwv"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "trainer.log_metrics(\"test\", test_results)\n",
        "trainer.save_metrics(\"test\", test_results)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCV83SVG3kDo"
      },
      "source": [
        "You can now upload the result of the training to the Hub, just execute this instruction (note that the Trainer will automatically create a model card as well as Tensorboard logs - see the \"Training metrics\" tab - amazing isn't it?):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkNll31Fn29k"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPacV3EX3qWT"
      },
      "source": [
        "Now that our model is trained, let's use it to run inference on a video from `test_dataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSSgIGsf3yqX"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68858mLQ30VD"
      },
      "source": [
        "Let's load the trained model checkpoint and fetch a video from `test_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2EGrIj52NOjr"
      },
      "outputs": [],
      "source": [
        "trained_model = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_yObs_zlflE",
        "outputId": "4f669651-763c-414d-cf7e-73e1d6d79a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video torch.Size([3, 16, 224, 224])\n",
            "video_name v_Basketball_g18_c04.avi\n",
            "video_index 57\n",
            "clip_index 0\n",
            "aug_index 0\n",
            "label 7\n",
            "Video label: Basketball\n"
          ]
        }
      ],
      "source": [
        "sample_test_video = next(iter(test_dataset))\n",
        "investigate_video(sample_test_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGJHV26538OU"
      },
      "source": [
        "We then prepare the video as a `torch.Tensor` and run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JdlpFA34mp1O"
      },
      "outputs": [],
      "source": [
        "def run_inference(model, video):\n",
        "    \"\"\"Utility to run inference given a model and test video.\n",
        "\n",
        "    The video is assumed to be preprocessed already.\n",
        "    \"\"\"\n",
        "    # (num_frames, num_channels, height, width)\n",
        "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
        "\n",
        "    inputs = {\n",
        "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
        "        \"labels\": torch.tensor(\n",
        "            [sample_test_video[\"label\"]]\n",
        "        ),  # this can be skipped if you don't have labels available.\n",
        "    }\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model = model.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VideoMAEModel.from_pretrained(\n",
        "    model_ckpt,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    output_attentions=True,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ],
      "metadata": {
        "id": "Qb3GjUFXvsJM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = sample_test_video[\"video\"]\n",
        "perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
        "inputs = {\n",
        "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
        "    }\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "model = model.to(device)\n",
        "with torch.no_grad():\n",
        "        outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "ZdCLRdqDug4D",
        "outputId": "7e64bce1-45e2-40f9-9d1a-28f6f75bce49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn = outputs.attentions"
      ],
      "metadata": {
        "id": "5GrNshpZuwN2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn[0].detach()"
      ],
      "metadata": {
        "id": "JmjpzJQ8xSzH",
        "outputId": "32aa8cde-127d-4776-df59-555f3eb54478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9.2701e-04, 1.1799e-03, 1.5372e-03,  ..., 4.5615e-04,\n",
              "           2.3543e-04, 2.7991e-04],\n",
              "          [9.4979e-04, 1.4531e-03, 2.0289e-03,  ..., 4.1473e-04,\n",
              "           1.9139e-04, 2.2262e-04],\n",
              "          [9.5620e-04, 1.4994e-03, 2.5855e-03,  ..., 3.6133e-04,\n",
              "           1.5095e-04, 1.7671e-04],\n",
              "          ...,\n",
              "          [7.6667e-04, 7.7155e-04, 7.6978e-04,  ..., 1.6056e-03,\n",
              "           4.3647e-04, 3.1542e-04],\n",
              "          [7.5325e-04, 7.5416e-04, 7.9539e-04,  ..., 5.4633e-04,\n",
              "           5.5891e-04, 5.7243e-04],\n",
              "          [7.6041e-04, 7.5517e-04, 8.3495e-04,  ..., 3.0462e-04,\n",
              "           4.1647e-04, 6.3208e-04]],\n",
              "\n",
              "         [[5.0697e-04, 6.0351e-04, 6.8090e-04,  ..., 2.7052e-04,\n",
              "           1.0085e-04, 2.8177e-04],\n",
              "          [7.1770e-04, 1.3450e-03, 1.4050e-03,  ..., 3.0568e-04,\n",
              "           1.2414e-04, 3.0708e-04],\n",
              "          [5.3507e-04, 9.9414e-04, 1.3427e-03,  ..., 1.9924e-04,\n",
              "           8.3249e-05, 1.9331e-04],\n",
              "          ...,\n",
              "          [9.6079e-04, 8.8484e-04, 9.5868e-04,  ..., 6.1940e-03,\n",
              "           2.2194e-03, 1.6553e-03],\n",
              "          [3.2322e-03, 3.5559e-03, 3.7739e-03,  ..., 1.9913e-02,\n",
              "           1.5017e-02, 8.0595e-03],\n",
              "          [2.8320e-03, 2.8878e-03, 3.2316e-03,  ..., 1.3110e-02,\n",
              "           9.2232e-03, 7.0749e-03]],\n",
              "\n",
              "         [[2.0835e-04, 1.6782e-04, 1.6867e-04,  ..., 4.2775e-07,\n",
              "           8.2752e-07, 7.0360e-07],\n",
              "          [1.8317e-09, 2.0196e-09, 2.3844e-09,  ..., 8.7844e-07,\n",
              "           8.5845e-07, 1.2361e-06],\n",
              "          [3.8562e-07, 4.9785e-07, 5.2380e-07,  ..., 4.6473e-05,\n",
              "           5.1460e-05, 5.5766e-05],\n",
              "          ...,\n",
              "          [8.9673e-09, 5.3625e-09, 4.2942e-09,  ..., 1.2488e-08,\n",
              "           5.4480e-09, 1.8360e-09],\n",
              "          [3.1156e-06, 2.7161e-06, 2.3315e-06,  ..., 5.1611e-06,\n",
              "           2.9713e-06, 1.3465e-06],\n",
              "          [4.9284e-09, 6.9548e-09, 6.4287e-09,  ..., 1.2720e-04,\n",
              "           8.3580e-05, 6.2652e-05]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[9.6952e-04, 1.3946e-03, 2.3825e-03,  ..., 1.4820e-04,\n",
              "           8.4291e-05, 2.2662e-04],\n",
              "          [5.6444e-04, 1.1097e-03, 2.1075e-03,  ..., 6.5752e-05,\n",
              "           4.0995e-05, 1.2104e-04],\n",
              "          [5.0439e-04, 1.0112e-03, 2.6707e-03,  ..., 5.7105e-05,\n",
              "           3.0321e-05, 8.8904e-05],\n",
              "          ...,\n",
              "          [1.1588e-04, 1.1184e-04, 2.1141e-04,  ..., 1.7891e-04,\n",
              "           1.1333e-04, 2.2409e-04],\n",
              "          [6.7339e-05, 7.3127e-05, 1.4094e-04,  ..., 6.4408e-05,\n",
              "           5.6160e-05, 1.2074e-04],\n",
              "          [1.1768e-04, 1.2956e-04, 2.4367e-04,  ..., 1.0152e-04,\n",
              "           4.6663e-05, 1.3801e-04]],\n",
              "\n",
              "         [[2.5107e-03, 2.6496e-03, 3.8006e-03,  ..., 6.1934e-05,\n",
              "           1.3113e-04, 2.2237e-04],\n",
              "          [2.8497e-03, 3.3712e-03, 5.0460e-03,  ..., 5.0229e-05,\n",
              "           1.1602e-04, 1.8751e-04],\n",
              "          [3.1893e-03, 3.9853e-03, 6.8695e-03,  ..., 3.9051e-05,\n",
              "           9.3859e-05, 1.4936e-04],\n",
              "          ...,\n",
              "          [7.5874e-04, 6.5147e-04, 7.6107e-04,  ..., 2.7372e-04,\n",
              "           5.6901e-04, 8.4083e-04],\n",
              "          [7.2312e-04, 6.4017e-04, 7.0450e-04,  ..., 2.9935e-04,\n",
              "           7.6249e-04, 1.0032e-03],\n",
              "          [7.6515e-04, 6.7891e-04, 7.4933e-04,  ..., 3.0176e-04,\n",
              "           7.1805e-04, 9.9381e-04]],\n",
              "\n",
              "         [[1.5049e-04, 8.4976e-05, 4.0609e-05,  ..., 7.0394e-05,\n",
              "           6.8099e-05, 1.1957e-04],\n",
              "          [8.8203e-05, 4.8826e-05, 2.0596e-05,  ..., 5.8591e-05,\n",
              "           6.4573e-05, 1.2028e-04],\n",
              "          [6.0676e-05, 3.2394e-05, 1.2500e-05,  ..., 5.8240e-05,\n",
              "           6.9077e-05, 1.3008e-04],\n",
              "          ...,\n",
              "          [5.5992e-04, 3.8682e-04, 2.7288e-04,  ..., 1.6741e-04,\n",
              "           1.3443e-04, 1.8953e-04],\n",
              "          [5.8562e-04, 4.3260e-04, 3.0617e-04,  ..., 2.2762e-04,\n",
              "           1.5253e-04, 2.6022e-04],\n",
              "          [5.3349e-04, 3.7455e-04, 2.6808e-04,  ..., 2.0927e-04,\n",
              "           1.6002e-04, 2.3347e-04]]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_matrix = attn[0][0][0].detach().numpy()"
      ],
      "metadata": {
        "id": "jBItLlmWxADC",
        "outputId": "aa8a83d1-f7a9-438d-b431-37c0c99a1607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-51-2926246900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ht-hpKLBnbS7"
      },
      "outputs": [],
      "source": [
        "logits = run_inference(trained_model, sample_test_video[\"video\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "id": "nKoe4SHWuUTP",
        "outputId": "bbdc91a3-affc-4515-91e9-439b5c73bdc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0292,  0.0157, -0.4346,  0.1700,  0.0451,  0.3213,  0.0267, -0.1130,\n",
              "          0.0362,  0.0038]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ3sdzzx4De-"
      },
      "source": [
        "We can now check if the model got the prediction right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4gF4pIjoXPO"
      },
      "outputs": [],
      "source": [
        "display_gif(sample_test_video[\"video\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE8TdTV_o9cm"
      },
      "outputs": [],
      "source": [
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdxWO1qO4PtH"
      },
      "source": [
        "And it looks like it got it right!\n",
        "\n",
        "You can also use this model to bring in your own videos. Check out [this Space](https://huggingface.co/spaces/sayakpaul/video-classification-ucf101-subset) to know more. The Space will also show you how to run inference for a single video file.\n",
        "\n",
        "<br><div align=center>\n",
        "    <img src=\"https://i.ibb.co/7nW4Rkn/sample-results.gif\" width=700/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWxR5SOwAFDo"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now that you've learned to train a well-performing video classification model on a custom dataset here is some homework for you:\n",
        "\n",
        "* Increase the dataset size: include more classes and more samples per class.\n",
        "* Try out different hyperparameters to study how the model converges.\n",
        "* Analyze the classes for which the model fails to perform well.\n",
        "* Try out a different video encoder.\n",
        "\n",
        "Don't forget to share your models with the community =)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1363b1b39aa644879c304cce686392e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eb2f8f98e8e408eb2d44b6174dc61c9",
              "IPY_MODEL_14e654de976c4e59ac3805f4aef80605",
              "IPY_MODEL_68fb73f8fa9e4c5587db390768c06902"
            ],
            "layout": "IPY_MODEL_c68c0a91c97641859c381f0e0ce4ee87"
          }
        },
        "2eb2f8f98e8e408eb2d44b6174dc61c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec7b5eadb3ab40ad9f2bf1bfcf7422af",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_02da0562b2d64314aeb8a368cf224a08",
            "value": "UCF101_subset.tar.gz:‚Äá100%"
          }
        },
        "14e654de976c4e59ac3805f4aef80605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f53deb79be5c40b6a00948969b4f1a1e",
            "max": 171386880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dc62a08141b479bb2aef681af472cde",
            "value": 171386880
          }
        },
        "68fb73f8fa9e4c5587db390768c06902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812352feeeec47829391aacbd2bccb3c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2757c1881dff427faf7a7b697b9c1066",
            "value": "‚Äá171M/171M‚Äá[00:04&lt;00:00,‚Äá61.0MB/s]"
          }
        },
        "c68c0a91c97641859c381f0e0ce4ee87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec7b5eadb3ab40ad9f2bf1bfcf7422af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02da0562b2d64314aeb8a368cf224a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53deb79be5c40b6a00948969b4f1a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dc62a08141b479bb2aef681af472cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "812352feeeec47829391aacbd2bccb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2757c1881dff427faf7a7b697b9c1066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aac3d030d04f46dab650c46be73742e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8e95293d2a64794990d9bf7baed574a",
              "IPY_MODEL_c92d7b5f70be49dc9517320e3b68ccd8",
              "IPY_MODEL_fc0dea2d2fe543c99e91213f06bff086"
            ],
            "layout": "IPY_MODEL_74be2b5c59354d4183f6eb79e031a29a"
          }
        },
        "e8e95293d2a64794990d9bf7baed574a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b092fd4502dc48d4bfc495acc8ed9970",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_400002c938c046c98940a7cc353f75d9",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "c92d7b5f70be49dc9517320e3b68ccd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f82cc11214784bd192f52a400042f6f9",
            "max": 271,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ae9ee7cd3854ce4ab827f09786b9833",
            "value": 271
          }
        },
        "fc0dea2d2fe543c99e91213f06bff086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9385c2381ce647879105a0517b45304a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fb93fef6438c45228caeac789837b49d",
            "value": "‚Äá271/271‚Äá[00:00&lt;00:00,‚Äá21.6kB/s]"
          }
        },
        "74be2b5c59354d4183f6eb79e031a29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b092fd4502dc48d4bfc495acc8ed9970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400002c938c046c98940a7cc353f75d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f82cc11214784bd192f52a400042f6f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae9ee7cd3854ce4ab827f09786b9833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9385c2381ce647879105a0517b45304a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb93fef6438c45228caeac789837b49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9c93c7baea74278b1383648615f663d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed535130915946e69820e55ff60a6c31",
              "IPY_MODEL_fedddfab7d8241c2bd123b58470d472f",
              "IPY_MODEL_9cc9d3e48b4e4270a6ba7df0fbafc9c5"
            ],
            "layout": "IPY_MODEL_ede2fde912ec4da78edfd1da9c683bd8"
          }
        },
        "ed535130915946e69820e55ff60a6c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e27e17290d3459a92a101e8a97f860f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_99a527c3700c4eb3929b4f36968b7c24",
            "value": "config.json:‚Äá100%"
          }
        },
        "fedddfab7d8241c2bd123b58470d472f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4959cacabdec4d16a35d7bc754146531",
            "max": 725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e8881830aac4be3908b5b206fd78691",
            "value": 725
          }
        },
        "9cc9d3e48b4e4270a6ba7df0fbafc9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514f929928d0490c83ffe4b39b48cfd3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_659ac7ede6ba4197b953754c97537882",
            "value": "‚Äá725/725‚Äá[00:00&lt;00:00,‚Äá57.6kB/s]"
          }
        },
        "ede2fde912ec4da78edfd1da9c683bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e27e17290d3459a92a101e8a97f860f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a527c3700c4eb3929b4f36968b7c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4959cacabdec4d16a35d7bc754146531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8881830aac4be3908b5b206fd78691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "514f929928d0490c83ffe4b39b48cfd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659ac7ede6ba4197b953754c97537882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "265db1817c304fe5b7d77808a53dc10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8acb16968b64b71a480503b2bc3b527",
              "IPY_MODEL_b4012293c0c649c4893b9eb711dca0d4",
              "IPY_MODEL_96e7b2da779f4ebb82ab47b1ee1658a4"
            ],
            "layout": "IPY_MODEL_a765714fd9c249ff95eace72695db977"
          }
        },
        "c8acb16968b64b71a480503b2bc3b527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aaa7828d31946678818cdd0761d349d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d356a1f0cb70435e8ddff2558accdcc8",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "b4012293c0c649c4893b9eb711dca0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86e8f2144cc4ec1bb549419dfc5ed9c",
            "max": 376873760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5c6335bebfd4571814468ba5bde0469",
            "value": 376873760
          }
        },
        "96e7b2da779f4ebb82ab47b1ee1658a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb56d881585e42a0a447331ba58c1787",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ac89b7ca8384145b9e405626992f1df",
            "value": "‚Äá377M/377M‚Äá[00:01&lt;00:00,‚Äá272MB/s]"
          }
        },
        "a765714fd9c249ff95eace72695db977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aaa7828d31946678818cdd0761d349d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d356a1f0cb70435e8ddff2558accdcc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c86e8f2144cc4ec1bb549419dfc5ed9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5c6335bebfd4571814468ba5bde0469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb56d881585e42a0a447331ba58c1787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac89b7ca8384145b9e405626992f1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}